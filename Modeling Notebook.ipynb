{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, BayesianRidge\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor, ARDRegression, SGDRegressor, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.max_colwidth', 999)\n",
    "\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee = pd.read_csv('./working_coffee_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price_Change</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Rate_of_Change</th>\n",
       "      <th>CV_Vectors</th>\n",
       "      <th>TFIDF_Vectors</th>\n",
       "      <th>Hash_Vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>IEG Vu: Easter email alert schedule, customer support hours Weekly Review: Funds still favor coffee shorts Futures Review: Robusta coffee drops in low volume Brazilian coffee exports seen at 35 mln 60-kg bags</td>\n",
       "      <td>1.1239</td>\n",
       "      <td>-0.0023</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.002042</td>\n",
       "      <td>[0 0 0 ... 0 0 0]</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>2018-04-03</td>\n",
       "      <td>Futures Review: Arabica coffee hits 10-month low Central American coffee exports mixed in March Brazilian green coffee exports down y-o-y Ivory Coast and Indonesian coffee exports fell sharply</td>\n",
       "      <td>1.1262</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>[0 0 0 ... 0 0 0]</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  \\\n",
       "1463  2018-03-29   \n",
       "1464  2018-04-03   \n",
       "\n",
       "                                                                                                                                                                                                                 Title  \\\n",
       "1463  IEG Vu: Easter email alert schedule, customer support hours Weekly Review: Funds still favor coffee shorts Futures Review: Robusta coffee drops in low volume Brazilian coffee exports seen at 35 mln 60-kg bags   \n",
       "1464                  Futures Review: Arabica coffee hits 10-month low Central American coffee exports mixed in March Brazilian green coffee exports down y-o-y Ivory Coast and Indonesian coffee exports fell sharply   \n",
       "\n",
       "       Price  Price_Change  Direction  Rate_of_Change         CV_Vectors  \\\n",
       "1463  1.1239       -0.0023          0       -0.002042  [0 0 0 ... 0 0 0]   \n",
       "1464  1.1262        0.0023          0        0.002046  [0 0 0 ... 0 0 0]   \n",
       "\n",
       "                TFIDF_Vectors             Hash_Vectors  \n",
       "1463  [0. 0. 0. ... 0. 0. 0.]  [0. 0. 0. ... 0. 0. 0.]  \n",
       "1464  [0. 0. 0. ... 0. 0. 0.]  [0. 0. 0. ... 0. 0. 0.]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (14,10))\n",
    "# plt.plot(coffee['Price'], label='Price')\n",
    "# plt.plot(coffee['Price'].rolling(30).mean(), label='30 Day MA')\n",
    "# plt.plot(coffee['Price'].rolling(50).mean(), label = '50 Day MA')\n",
    "# plt.xlabel(\"Days\")\n",
    "# plt.ylabel('Price $USD/ Pound')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "#tf = TfidfVectorizer()\n",
    "#lr = LinearRegression()\n",
    "svr = svm.SVR()\n",
    "\n",
    "X = coffee[['Title', 'Price_Change', 'Rate_of_Change']]\n",
    "y = coffee['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Need to create a vector, can't give it a DF.  \n",
    "# Isolate the title\n",
    "\n",
    "def title(df):\n",
    "    return df['Title']\n",
    "\n",
    "title_tf = FunctionTransformer(title, validate=False)    \n",
    "\n",
    "text_pipeline_cv = Pipeline([\n",
    "    \n",
    "    ('title_tf', title_tf),\n",
    "    ('cv', cv),\n",
    "    ('svr', svr)\n",
    "    \n",
    "])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    'cv__stop_words': [None,'english'],\n",
    "    'cv__ngram_range' : [(1,2), (1,2), (1,3)],\n",
    "    'cv__max_features' : [None, 1000],\n",
    "    'cv__binary' : [True, False],\n",
    "    'cv__min_df' : [1,2,3,],\n",
    "    'svr__kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'svr__shrinking' : [True, False],\n",
    "    'svr__gamma' : ['auto', 1.0, 2.0]\n",
    "    \n",
    "}\n",
    "gs = GridSearchCV(text_pipeline_cv, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Passive Agressive Regressor Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "#tf = TfidfVectorizer()\n",
    "#lr = LinearRegression()\n",
    "pa = PassiveAggressiveRegressor()\n",
    "\n",
    "X = coffee[['Title', 'Rate_of_Change', 'Price_Change']]\n",
    "y = coffee['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "# Need to create a vector, can't give it a DF.  \n",
    "# Isolate the title\n",
    "\n",
    "def title(df):\n",
    "    return df['Title']\n",
    "\n",
    "title_tf = FunctionTransformer(title, validate=False)    \n",
    "\n",
    "text_pipeline_cv = Pipeline([\n",
    "    \n",
    "    ('title_tf', title_tf),\n",
    "    ('cv', cv),\n",
    "    ('pa', pa)\n",
    "    \n",
    "])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    'cv__stop_words': [None,'english'],\n",
    "    'cv__ngram_range' : [(1,2), (1,2), (1,3), (2,3)],\n",
    "    'cv__max_features' : [None, 1000, 1500, 2000],\n",
    "    'cv__binary' : [True, False],\n",
    "    'cv__min_df' : [1,2,3],\n",
    "    'cv__max_df' : [1.0, .75],\n",
    "    'pa__n_iter': [5,10,15,20,25,30,35,40],\n",
    "    'pa__loss' : ['squared_epsilon_insensitive', 'epsilon_insensitive'],\n",
    "    'pa__C' : [1.0, .5, .25, .125, .0675]\n",
    "    \n",
    "}\n",
    "gs = GridSearchCV(text_pipeline_cv, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARD Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "#tf = TfidfVectorizer()\n",
    "#lr = LinearRegression()\n",
    "ard = ARDRegression()\n",
    "to_dense = DenseTransformer()\n",
    "\n",
    "X = coffee[['Title', 'Rate_of_Change', 'Price_Change']]\n",
    "y = coffee['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "# Need to create a vector, can't give it a DF.  \n",
    "# Isolate the title\n",
    "\n",
    "def title(df):\n",
    "    return df['Title']\n",
    "\n",
    "\n",
    "title_tf = FunctionTransformer(title, validate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_pipeline_ard = Pipeline([\n",
    "    \n",
    "    ('title_tf', title_tf),\n",
    "    ('cv', cv),\n",
    "    ('to_dense', DenseTransformer()),\n",
    "    ('ard', ard)\n",
    "    \n",
    "])\n",
    "\n",
    "# params = {\n",
    "    \n",
    "#     'cv__stop_words': [None,'english'],\n",
    "#     'cv__ngram_range' : [(1,2), (1,2), (1,3), (2,3)],\n",
    "#     'cv__max_features' : [None, 1000, 1500],\n",
    "#     'cv__binary' : [True, False],\n",
    "#     'cv__min_df' : [1,2,3,4],\n",
    "#     'cv__max_df' : [1.0, .75],\n",
    "#     'ard__n_iter' : [300,400,500,600],\n",
    "#     'ard__compute_score' : [True, False],\n",
    "#     'ard__tol' : [.001,.0001,.00001]\n",
    "    \n",
    "# }\n",
    "# gs = GridSearchCV(text_pipeline_ard, param_grid=params)\n",
    "# gs.fit(X_train, y_train)\n",
    "# print(gs.best_score_)\n",
    "# gs.best_params_\n",
    "\n",
    "\n",
    "results = text_pipeline_ard.fit(X_train, y_train)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "#tf = TfidfVectorizer()\n",
    "#lr = LinearRegression()\n",
    "sgdreg = SGDRegressor()\n",
    "\n",
    "X = coffee[['Title', 'Date']]\n",
    "y = coffee['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "# Need to create a vector, can't give it a DF.  \n",
    "# Isolate the title\n",
    "\n",
    "def title(df):\n",
    "    return df['Title']\n",
    "\n",
    "\n",
    "title_tf = FunctionTransformer(title, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline_bayes = Pipeline([\n",
    "    \n",
    "    ('title_tf', title_tf),\n",
    "    ('cv', cv),\n",
    "    ('sgdreg', sgdreg)\n",
    "    \n",
    "])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    'cv__stop_words': [None,'english'],\n",
    "    'cv__ngram_range' : [(1,2), (1,2), (1,3), (2,3)],\n",
    "    'cv__max_features' : [None, 1000, 1500],\n",
    "    'cv__binary' : [True, False],\n",
    "    'cv__min_df' : [1,2,3,4],\n",
    "    'cv__max_df' : [1.0, .75],\n",
    "    'sgdreg__loss' : ['squared_loss', 'huber', 'epsilon_insensitive','squared_epsilon_insensitive'],\n",
    "    'sgdreg__penalty' : ['none', 'l2', 'l1','elasticnet'],\n",
    "    'sgdreg__n_iter' : [5,10,15]\n",
    "    \n",
    "    \n",
    "}\n",
    "gs = GridSearchCV(text_pipeline_bayes, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "#tf = TfidfVectorizer()\n",
    "#lr = LinearRegression()\n",
    "tree = DecisionTreeRegressor()\n",
    "\n",
    "X = coffee[['Title', 'Price_Change', 'Rate_of_Change']]\n",
    "y = coffee['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "# Need to create a vector, can't give it a DF.  \n",
    "# Isolate the title\n",
    "\n",
    "def title(df):\n",
    "    return df['Title']\n",
    "\n",
    "\n",
    "title_tf = FunctionTransformer(title, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline_tree = Pipeline([\n",
    "    \n",
    "    ('title_tf', title_tf),\n",
    "    ('cv', cv),\n",
    "    ('tree', tree)\n",
    "    \n",
    "])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    'cv__stop_words': [None,'english'],\n",
    "    'cv__ngram_range' : [(1,2), (1,2), (1,3)],\n",
    "    'cv__max_features' : [None, 1000, 1500],\n",
    "    'cv__min_df' : [1,2,3],\n",
    "    'tree__criterion' : ['mse', 'mae'],\n",
    "    'tree__max_features' : ['auto', 'log2', 'sqrt'],\n",
    "    'tree__max_depth' : [5, 10]\n",
    "    \n",
    "    \n",
    "}\n",
    "gs = GridSearchCV(text_pipeline_tree, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baysean Regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee = pd.read_csv('./updated_coffee_df.csv')\n",
    "\n",
    "coffee.head()\n",
    "coffee.drop(coffee[['Unnamed: 0', 'Unnamed: 0.1']], axis=1, inplace=True)\n",
    "\n",
    "coffee['Date'] = pd.to_datetime(coffee['Date'])\n",
    "coffee.head()\n",
    "\n",
    "#tf = TfidfVectorizer()\n",
    "#lr = LinearRegression()\n",
    "bayes = BayesianRidge()\n",
    "\n",
    "X = coffee['Title']\n",
    "y = coffee['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "ss = StandardScaler()\n",
    "\n",
    "#need to vectorize the text first.\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "\n",
    "# encode document\n",
    "X_train_vector = vectorizer.transform(X_train)\n",
    "X_test_vector = vectorizer.transform(X_test)\n",
    "# summarize encoded vector\n",
    "# print(vector.shape)\n",
    "# print(vector.toarray())\n",
    "\n",
    "\n",
    "X_train_scaled = ss.fit_transform(X_train_vector.toarray())\n",
    "X_test_scaled = ss.transform(X_test_vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Rgression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "#tf = TfidfVectorizer()\n",
    "#lr = LinearRegression()\n",
    "knn = KNeighborsRegressor() \n",
    "\n",
    "X = coffee[['Title', 'Date', 'Price_Change', 'Rate_of_Change']]\n",
    "y = coffee['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "# Need to create a vector, can't give it a DF.  \n",
    "# Isolate the title\n",
    "\n",
    "def title(df):\n",
    "    return df['Title']\n",
    "\n",
    "\n",
    "title_tf = FunctionTransformer(title, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline_knn = Pipeline([\n",
    "    \n",
    "    ('title_tf', title_tf),\n",
    "    ('cv', cv),\n",
    "    ('knn', knn)\n",
    "    \n",
    "])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    'cv__stop_words': [None,'english'],\n",
    "    'cv__ngram_range' : [(1,2), (1,2), (1,3)],\n",
    "    'cv__max_features' : [None, 1000, 1500],\n",
    "    'cv__min_df' : [1,2,3],\n",
    "    'knn__n_neighbors' : [1,2,3,4,5,6],\n",
    "    'knn__algorithm' : ['auto', 'brute'],\n",
    "    'knn__weights' : ['uniform', 'distance']\n",
    "    \n",
    "    \n",
    "    \n",
    "}\n",
    "gs = GridSearchCV(text_pipeline_knn, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARD Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee = pd.read_csv('./updated_coffee_df.csv')\n",
    "\n",
    "coffee.head()\n",
    "coffee.drop(coffee[['Unnamed: 0', 'Unnamed: 0.1']], axis=1, inplace=True)\n",
    "\n",
    "coffee['Date'] = pd.to_datetime(coffee['Date'])\n",
    "coffee.head()\n",
    "\n",
    "#tf = TfidfVectorizer()\n",
    "#lr = LinearRegression()\n",
    "ard = ARDRegression()\n",
    "\n",
    "X = coffee['Title']\n",
    "y = coffee['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "ss = StandardScaler()\n",
    "\n",
    "#need to vectorize the text first.\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "\n",
    "# encode document\n",
    "X_train_vector = vectorizer.transform(X_train)\n",
    "X_test_vector = vectorizer.transform(X_test)\n",
    "# summarize encoded vector\n",
    "# print(vector.shape)\n",
    "# print(vector.toarray())\n",
    "\n",
    "\n",
    "X_train_scaled = ss.fit_transform(X_train_vector.toarray())\n",
    "X_test_scaled = ss.transform(X_test_vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ard.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ard.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ard.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regresssion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "#tf = TfidfVectorizer()\n",
    "#lr = LinearRegression()\n",
    "ridge = Ridge()\n",
    "\n",
    "X = coffee[['Title','Price_Change', 'Rate_of_Change']]\n",
    "y = coffee['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "# Need to create a vector, can't give it a DF.  \n",
    "# Isolate the title\n",
    "\n",
    "def title(df):\n",
    "    return df['Title']\n",
    "\n",
    "\n",
    "title_tf = FunctionTransformer(title, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline_ridge = Pipeline([\n",
    "    \n",
    "    ('title_tf', title_tf),\n",
    "    ('cv', cv),\n",
    "    ('ridge', ridge)\n",
    "    \n",
    "])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    'cv__stop_words': [None],\n",
    "    'cv__ngram_range' : [(1,3), (2,3), (3,4), (4,5)],\n",
    "    'cv__max_features' : [1000, 1500, 2000],\n",
    "    'cv__min_df' : [1,2,3],\n",
    "    #'cv__norm' : ['l1', 'l2', None],\n",
    "    'ridge__alpha': [ 4., 4.5, 5, 5.5, 6.0, 6.5],\n",
    "    'ridge__max_iter': [2500, 2750, 2800, 2900],\n",
    "    'ridge__solver' : ['auto','cholesky'],\n",
    "    'ridge__normalize': [True, False],\n",
    "    'ridge__tol' : [.01, .001, .0001]\n",
    "    \n",
    "    \n",
    "    \n",
    "}\n",
    "gs = GridSearchCV(text_pipeline_ridge, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###cv = CountVectorizer()\n",
    "tf = TfidfVectorizer()\n",
    "#lr = LinearRegression()\n",
    "lasso = Lasso()\n",
    "\n",
    "X = coffee[['Title', 'Date', 'Price_Change', 'Rate_of_Change']]\n",
    "y = coffee['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "# Need to create a vector, can't give it a DF.  \n",
    "# Isolate the title\n",
    "\n",
    "def title(df):\n",
    "    return df['Title']\n",
    "\n",
    "\n",
    "title_tf = FunctionTransformer(title, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline_lasso = Pipeline([\n",
    "    \n",
    "    ('title_tf', title_tf),\n",
    "    ('cv', cv),\n",
    "    ('lasso', lasso)\n",
    "    \n",
    "])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    'cv__stop_words': [None,'english'],\n",
    "    'cv__ngram_range' : [(1,2), (1,2), (1,3)],\n",
    "    'cv__max_features' : [None, 1000, 1500],\n",
    "    'cv__min_df' : [1,2,3],\n",
    "    \n",
    "    'lasso__max_iter': [1000, 2000, 3000, 4000],\n",
    "    'lasso__tol' : [.001, .0001, .00001]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "}\n",
    "gs = GridSearchCV(text_pipeline_lasso, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "#tf = TfidfVectorizer()\n",
    "#lr = LinearRegression()\n",
    "enet = ElasticNet()\n",
    "\n",
    "X = coffee[['Title', 'Date', 'Price_Change', 'Rate_of_Change']]\n",
    "y = coffee['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "# Need to create a vector, can't give it a DF.  \n",
    "# Isolate the title\n",
    "\n",
    "def title(df):\n",
    "    return df['Title']\n",
    "\n",
    "\n",
    "title_tf = FunctionTransformer(title, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline_enet = Pipeline([\n",
    "    \n",
    "    ('title_tf', title_tf),\n",
    "    ('cv', cv),\n",
    "    ('enet', enet)\n",
    "    \n",
    "])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    'cv__stop_words': [None,'english'],\n",
    "    'cv__ngram_range' : [(1,2), (1,3), (1,4)],\n",
    "    'cv__max_features' : [None, 1000, 1500],\n",
    "    'cv__min_df' : [1,2,3],\n",
    "    'enet__alpha': [1., 1.5, 2.0, 2.5],\n",
    "    'enet__l1_ratio' : [.25, .5, .75],\n",
    "    'enet__precompute' : [True, False],\n",
    "    'enet__max_iter' : [1000,1500, 2000, 2500]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "}\n",
    "gs = GridSearchCV(text_pipeline_enet, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price_Change</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Rate_of_Change</th>\n",
       "      <th>CV_Vectors</th>\n",
       "      <th>TFIDF_Vectors</th>\n",
       "      <th>Hash_Vectors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-02</th>\n",
       "      <td>India earns more from higher coffee exports in 2006</td>\n",
       "      <td>1.1506</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0 0 0 ... 0 0 0]</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>Friesland raises stake in Indonesian subsidiary MILK PRODUCT PRICES IN EUROPE AND THE US Milk product prices, Jan 3 Starbucks reveals trans fat removal plan Indonesian coffee shipments stable after floods</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022075</td>\n",
       "      <td>[0 0 0 ... 0 0 0]</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                    Title  \\\n",
       "Date                                                                                                                                                                                                                        \n",
       "2007-01-02                                                                                                                                                           India earns more from higher coffee exports in 2006    \n",
       "2007-01-03  Friesland raises stake in Indonesian subsidiary MILK PRODUCT PRICES IN EUROPE AND THE US Milk product prices, Jan 3 Starbucks reveals trans fat removal plan Indonesian coffee shipments stable after floods    \n",
       "\n",
       "             Price  Price_Change  Direction  Rate_of_Change  \\\n",
       "Date                                                          \n",
       "2007-01-02  1.1506        0.0000          0        0.000000   \n",
       "2007-01-03  1.1760        0.0254          0        0.022075   \n",
       "\n",
       "                   CV_Vectors            TFIDF_Vectors  \\\n",
       "Date                                                     \n",
       "2007-01-02  [0 0 0 ... 0 0 0]  [0. 0. 0. ... 0. 0. 0.]   \n",
       "2007-01-03  [0 0 0 ... 0 0 0]  [0. 0. 0. ... 0. 0. 0.]   \n",
       "\n",
       "                       Hash_Vectors  \n",
       "Date                                 \n",
       "2007-01-02  [0. 0. 0. ... 0. 0. 0.]  \n",
       "2007-01-03  [0. 0. 0. ... 0. 0. 0.]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = coffee[['Price_Change', 'Rate_of_Change', 'Title']]\n",
    "y = coffee['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "def title(df):\n",
    "    return df['Title']\n",
    "\n",
    "\n",
    "title_tf = FunctionTransformer(title, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41316977963716595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cv__max_features': 1500,\n",
       " 'cv__min_df': 2,\n",
       " 'cv__ngram_range': (1, 4),\n",
       " 'cv__stop_words': 'english',\n",
       " 'rf__max_depth': None,\n",
       " 'rf__max_features': 'sqrt',\n",
       " 'rf__n_estimators': 30,\n",
       " 'rf__n_jobs': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline_rfreg = Pipeline([\n",
    "    \n",
    "    ('title_tf', title_tf),\n",
    "    ('cv', cv),\n",
    "    ('rf', rf)\n",
    "    \n",
    "])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    'cv__stop_words': [None,'english'],\n",
    "    'cv__ngram_range' : [(1,2), (1,3), (1,4)],\n",
    "    'cv__max_features' : [1000, 1500, 2000],\n",
    "    'cv__min_df' : [1,2,3],\n",
    "    'rf__n_estimators': [10,20,30,40,50],\n",
    "    'rf__max_depth' : [None, 10, 15, 20],\n",
    "    'rf__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "    'rf__n_jobs' : [2]\n",
    "}\n",
    "gs = GridSearchCV(text_pipeline_rfreg, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "bag = BaggingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = coffee[['Price_Change', 'Rate_of_Change', 'Title']]\n",
    "y = coffee['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "def title(df):\n",
    "    return df['Title']\n",
    "\n",
    "\n",
    "title_tf = FunctionTransformer(title, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4288228067772207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bag__max_features': 0.5,\n",
       " 'bag__n_estimators': 50,\n",
       " 'bag__n_jobs': 2,\n",
       " 'cv__max_features': 1000,\n",
       " 'cv__min_df': 2,\n",
       " 'cv__ngram_range': (1, 3),\n",
       " 'cv__stop_words': 'english'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline_bag = Pipeline([\n",
    "    \n",
    "    ('title_tf', title_tf),\n",
    "    ('cv', cv),\n",
    "    ('bag', bag)\n",
    "    \n",
    "])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    'cv__stop_words': [None,'english'],\n",
    "    'cv__ngram_range' : [(1,2), (1,3), (1,4)],\n",
    "    'cv__max_features' : [1000, 1500, 2000],\n",
    "    'cv__min_df' : [1,2,3],\n",
    "    'bag__n_estimators' : [10,20,30,40,50],\n",
    "    'bag__max_features' : [1.0, .75, .5, .25],\n",
    "    'bag__n_jobs' : [2]\n",
    " \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(text_pipeline_bag, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
