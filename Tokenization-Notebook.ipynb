{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import six\n",
    "from abc import ABCMeta\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV \n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer     \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Input, Flatten \n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding \n",
    "from keras.models import Model \n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import load_model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.layers import Embedding\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import initializers \n",
    "from keras.layers import regularizers \n",
    "from keras.layers import constraints \n",
    "from keras.layers import Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.constraints import max_norm\n",
    "import keras.backend as K\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee = pd.read_csv('./updated_coffee_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>level_3</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price_Change</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Rate_of_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>India earns more from higher coffee exports in...</td>\n",
       "      <td>2007-01-02</td>\n",
       "      <td>1.1506</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>Friesland raises stake in Indonesian subsidiar...</td>\n",
       "      <td>2007-01-03</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>Nymex announces start date for soft commodity ...</td>\n",
       "      <td>2007-01-04</td>\n",
       "      <td>1.1451</td>\n",
       "      <td>-0.0309</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.026276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>India's largest coffee chain extends to Pakistan</td>\n",
       "      <td>2007-01-05</td>\n",
       "      <td>1.1506</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>Honduran coffee sales Ugandan coffee funds Soy...</td>\n",
       "      <td>2007-01-07</td>\n",
       "      <td>1.1506</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  year  month  day level_3  \\\n",
       "0           0             0  2007      1    2   Title   \n",
       "1           1             1  2007      1    3   Title   \n",
       "2           2             2  2007      1    4   Title   \n",
       "3           3             3  2007      1    5   Title   \n",
       "4           4             4  2007      1    7   Title   \n",
       "\n",
       "                                               Title       Date   Price  \\\n",
       "0  India earns more from higher coffee exports in... 2007-01-02  1.1506   \n",
       "1  Friesland raises stake in Indonesian subsidiar... 2007-01-03  1.1760   \n",
       "2  Nymex announces start date for soft commodity ... 2007-01-04  1.1451   \n",
       "3  India's largest coffee chain extends to Pakistan  2007-01-05  1.1506   \n",
       "4  Honduran coffee sales Ugandan coffee funds Soy... 2007-01-07  1.1506   \n",
       "\n",
       "   Price_Change  Direction  Rate_of_Change  \n",
       "0        0.0000          0        0.000000  \n",
       "1        0.0254          0        0.022075  \n",
       "2       -0.0309          0       -0.026276  \n",
       "3        0.0055          0        0.004803  \n",
       "4       -0.0000          0        0.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee['Date'] = pd.to_datetime(coffee['Date'])\n",
    "coffee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = coffee[['Title', 'Date', 'Price_Change', 'Rate_of_Change']]\n",
    "y = coffee['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "thedata = coffee\n",
    "thedata['Title'] = thedata['Title'].str.encode('utf-8')\n",
    "thedata['Title'] = thedata['Title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# clean up the text in the data with regex\n",
    "##########################################\n",
    "def clean_text(row):\n",
    "    text = str(row['Title'])\n",
    "\n",
    "    # Remove newline characters\n",
    "    cleantext = text.replace('\\r\\n', ' ')\n",
    "\n",
    "    # Convert HTML punctuation chaaracters\n",
    "    cleantext = cleantext.replace(' www.', ' ')   \n",
    "    cleantext = cleantext.replace('.com ', ' ')    \n",
    "    cleantext = cleantext.replace('.', ' ')\n",
    "    cleantext = cleantext.replace(',', ' ')\n",
    "    cleantext = cleantext.replace('!', ' ')\n",
    "    cleantext = cleantext.replace('$;', ' ')\n",
    "    cleantext = cleantext.replace(';', ' ')\n",
    "    cleantext = cleantext.replace(')', ' ')\n",
    "    cleantext = cleantext.replace('(', ' ')   \n",
    "    cleantext = cleantext.replace('>', ' ')  \n",
    "    cleantext = cleantext.replace('<', ' ')  \n",
    "    cleantext = cleantext.replace('-', ' ')  #take away hyphen and collapse hyphenated words\n",
    "    cleantext = cleantext.replace(' the ', ' ')  \n",
    "    cleantext = cleantext.replace(' of ', ' ')   \n",
    "    cleantext = cleantext.replace(' in ', ' ')  \n",
    "    cleantext = cleantext.replace(' and ', ' ')  \n",
    "    cleantext = cleantext.replace(' by ', ' ')  \n",
    "    cleantext = cleantext.replace(' to ', ' ')  \n",
    "    cleantext = cleantext.replace(' at ', ' ')  \n",
    "    cleantext = cleantext.replace(' on ', ' ') \n",
    "    cleantext = cleantext.replace(' for ', ' ')  \n",
    "#    cleantext = cleantext.replace(' be ', ' ')   \n",
    "    cleantext = cleantext.replace(' is ', ' ')    \n",
    "    cleantext = cleantext.replace(' or ', ' ')   \n",
    "#    cleantext = cleantext.replace(' we ', ' ')   \n",
    "#    cleantext = cleantext.replace(' that ', ' ')   \n",
    "#    cleantext = cleantext.replace(' our ', ' ')   \n",
    "    cleantext = cleantext.replace(' as ', ' ')        \n",
    "    cleantext = cleantext.replace(' from ', ' ')   \n",
    "#    cleantext = cleantext.replace(' are ', ' ')   \n",
    "    cleantext = cleantext.replace(' with ', ' ')   \n",
    "#    cleantext = cleantext.replace(' us ', ' ')   \n",
    "#    cleantext = cleantext.replace(' was ', ' ')        \n",
    "    cleantext = cleantext.replace(' this ', ' ')   \n",
    "    cleantext = cleantext.replace(' an ', ' ')        \n",
    "    cleantext = cleantext.replace(' by ', ' ')   \n",
    "    cleantext = cleantext.replace(' sr ', ' ')      \n",
    "    cleantext = cleantext.replace(' it ', ' ')  \n",
    "    cleantext = cleantext.replace(' s ', ' ')   \n",
    "\n",
    "\n",
    "    #remove non alpha characters and specific noise\n",
    "    cleantext = re.sub(r'\\d+', ' ',cleantext)\n",
    "    cleantext = re.sub(r'^b',' ',cleantext)\n",
    "    cleantext = re.sub(r'[^\\w]',' ',cleantext)\n",
    "    cleantext = cleantext.replace('xc xs', ' ')  \n",
    "    cleantext = cleantext.replace('xe xs', ' ')  \n",
    "    cleantext = cleantext.replace('xc xS', ' ')  \n",
    "    cleantext = cleantext.replace('xe xS', ' ')  \n",
    "    cleantext = cleantext.replace('xc xa', ' ')  \n",
    "    cleantext = cleantext.replace('xe xa', ' ')  \n",
    "    cleantext = cleantext.replace(' xc xc x', ' ')  \n",
    "    cleantext = cleantext.replace(' xc ', ' ')  \n",
    "    cleantext = cleantext.replace(' xe ', ' ')  \n",
    "    cleantext = cleantext.replace(' xs ', ' ')  \n",
    "    cleantext = cleantext.replace(' xa ', ' ')  \n",
    "    cleantext = cleantext.replace(' ct ', ' ')  \n",
    "    cleantext = cleantext.replace(' x ', ' ')  \n",
    "    cleantext = cleantext.replace(' non exclusive ', ' non-exclusive ') \n",
    "    cleantext = cleantext.replace(' u ', ' ')   \n",
    "    cleantext = cleantext.replace(' s ', ' ')  \n",
    "    \n",
    "    #remove specific noise\n",
    "    cleantext = cleantext.translate(str.maketrans({'‘':' ','’':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({',':' ',',':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({'[':' ',']':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({'\"':' ','%':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({'^':' ','*':' '}))\n",
    "\n",
    "    #remove punctuation\n",
    "    punctpattern = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    cleanttext = re.sub(punctpattern,'', cleantext)\n",
    "\n",
    "    #remove single letter word\n",
    "    cleantext = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', cleantext) \n",
    "\n",
    "    # Remove extra spaces\n",
    "    cleantext = re.sub('\\s+', ' ', cleantext).strip()\n",
    "\n",
    "    return cleantext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply text fixes to the input text column\n",
    "thedata['Clean_Text'] = thedata.apply(clean_text, axis=1)\n",
    "justcleandocs=thedata.drop(['Title'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>level_3</th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price_Change</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Rate_of_Change</th>\n",
       "      <th>Clean_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>2007-01-02</td>\n",
       "      <td>1.1506</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>india earns more higher coffee exports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>2007-01-03</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022075</td>\n",
       "      <td>friesland raises stake indonesian subsidiary m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>2007-01-04</td>\n",
       "      <td>1.1451</td>\n",
       "      <td>-0.0309</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.026276</td>\n",
       "      <td>nymex announces start date soft commodity futu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>2007-01-05</td>\n",
       "      <td>1.1506</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004803</td>\n",
       "      <td>india largest coffee chain extends pakistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>2007-01-07</td>\n",
       "      <td>1.1506</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>honduran coffee sales ugandan coffee funds soy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  year  month  day level_3       Date   Price  \\\n",
       "0           0             0  2007      1    2   Title 2007-01-02  1.1506   \n",
       "1           1             1  2007      1    3   Title 2007-01-03  1.1760   \n",
       "2           2             2  2007      1    4   Title 2007-01-04  1.1451   \n",
       "3           3             3  2007      1    5   Title 2007-01-05  1.1506   \n",
       "4           4             4  2007      1    7   Title 2007-01-07  1.1506   \n",
       "\n",
       "   Price_Change  Direction  Rate_of_Change  \\\n",
       "0        0.0000          0        0.000000   \n",
       "1        0.0254          0        0.022075   \n",
       "2       -0.0309          0       -0.026276   \n",
       "3        0.0055          0        0.004803   \n",
       "4       -0.0000          0        0.000000   \n",
       "\n",
       "                                          Clean_Text  \n",
       "0             india earns more higher coffee exports  \n",
       "1  friesland raises stake indonesian subsidiary m...  \n",
       "2  nymex announces start date soft commodity futu...  \n",
       "3        india largest coffee chain extends pakistan  \n",
       "4  honduran coffee sales ugandan coffee funds soy...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "justcleandocs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save a cleaned copy to inspect\n",
    "justcleandocs.to_csv('cleandata.tsv', sep='\\t', encoding='utf-8')\n",
    "justcleandocs.to_csv('cleandatacomma.tsv', sep=',', encoding='utf-8',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = justcleandocs['Clean_Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  india earns more higher coffee exports\n",
       "1       friesland raises stake indonesian subsidiary m...\n",
       "2       nymex announces start date soft commodity futu...\n",
       "3             india largest coffee chain extends pakistan\n",
       "4       honduran coffee sales ugandan coffee funds soy...\n",
       "5                mcdonald coffee be certified sustainable\n",
       "6       milk product prices europe us milk product pri...\n",
       "7       fall crude sees sugar hit three month low mcdo...\n",
       "8                       coke invests australian operation\n",
       "9       world coffee production fall below demand ico ...\n",
       "10      milk product prices europe us milk product pri...\n",
       "11      australian investment nestle scrutinise its in...\n",
       "12      starbucks denies wwf allegations over coffee p...\n",
       "13      weather fight acreage sends soyabeans higher h...\n",
       "14      officials ethiopia starbucks meet geneva ugand...\n",
       "15      milk product prices europe us monsanto concede...\n",
       "16      louis dreyfus brazil push selling elevator fir...\n",
       "17          indian coffee exports set rise over last year\n",
       "18      trading briefs taking advantage amazing demand...\n",
       "19      guatemala reports slump coffee exports europea...\n",
       "20      nestle pledges help improve vietnam coffee sec...\n",
       "21      cameroon uses insecticides fight caterpillar i...\n",
       "22      milk product prices february green mountain co...\n",
       "23      kenya airways begins using utz kapeh cerified ...\n",
       "24      dryness puts cocoa near seven month highs viet...\n",
       "25      brazilians be urged drink more milk alpina dis...\n",
       "26        cbk gives more upbeat view kenyan coffee output\n",
       "27      milk escapes health advertising ruling milk pr...\n",
       "28      cbk sees climb production uganda lifts exports...\n",
       "29      starbucks meets press dispute ethiopian storie...\n",
       "                              ...                        \n",
       "1428    vietnam coffee exports rise futures review cof...\n",
       "1429    weekly review coffee plenty soon less later fu...\n",
       "1430    brazil green coffee exports fell futures revie...\n",
       "1431           futures review coffee futures settle mixed\n",
       "1432    podcast how will global sugar prices fare futu...\n",
       "1433    brazil coffee output projected rise futures re...\n",
       "1434    weekly review coffee waits brazilian developme...\n",
       "1435    futures review arabica coffee drops month low ...\n",
       "1436    futures review robusta coffee hits week low th...\n",
       "1437           futures review robusta end up arabica down\n",
       "1438    brazil export more coffee futures review arabi...\n",
       "1439    weekly review brazil outlook weighs coffee pri...\n",
       "1440    futures review coffee prices settle mixed ieg ...\n",
       "1441    honduran coffee exports lower than expected fu...\n",
       "1442    netherlands trades organic food but does not b...\n",
       "1443    indonesian sumatra coffee exports take tumble ...\n",
       "1444    weekly review coffee producers struggling acce...\n",
       "1445    futures review asian export pace keeps arabica...\n",
       "1446    ieg vu february top sugar price forecast orang...\n",
       "1447    futures review robusta coffee values still fir...\n",
       "1448           futures review robusta prices settle mixed\n",
       "1449    weekly review will coffee get brazilian lift v...\n",
       "1450    futures review arabica coffee prices continue ...\n",
       "1451    futures review arabica coffee weakens again sl...\n",
       "1452    brazilian coffee crop weighs prices futures re...\n",
       "1453            futures review coffee prices settle lower\n",
       "1454    cocoa prices soar above coffee sugar weekly re...\n",
       "1455    us green coffee stocks fell month low futures ...\n",
       "1456    brazilian farmers sell coffee crop futures rev...\n",
       "1457    brazilian coffee prices see moderate fall futu...\n",
       "Name: Clean_Text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spot check\n",
    "documents[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################################\n",
    "#\n",
    "# Stock future performance classification based on text\n",
    "#\n",
    "# Approach:\n",
    "#\n",
    "# Build on top of it a 1D convolutional neural network, ending in a softmax output over 3 even categories.\n",
    "# Use word Glove word vectors for large English text corpus as inputs model\n",
    "#\n",
    "# Steps\n",
    "# 1) After cleaning, we convert all text samples in the dataset into sequences of word indices.  In this case, a \"word index\" would simply be an integer ID for the word. \n",
    "# 2) We consider the top 350,000 most commonly occuring words in the dataset\n",
    "# 3) We truncate the sequences to a maximum length of 25,000 words.\n",
    "# 5) We [repare an \"embedding matrix\" which will contain at index i the embedding vector for the word of index i in our word index.\n",
    "# 6) Then, we load this embedding matrix into a Keras Embedding layer, set to be frozen (its weights, the embedding vectors, will not be updated during training).\n",
    "#\n",
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post regex documents 0               india earns more higher coffee exports\n",
      "1    friesland raises stake indonesian subsidiary m...\n",
      "2    nymex announces start date soft commodity futu...\n",
      "3          india largest coffee chain extends pakistan\n",
      "4    honduran coffee sales ugandan coffee funds soy...\n",
      "5             mcdonald coffee be certified sustainable\n",
      "6    milk product prices europe us milk product pri...\n",
      "7    fall crude sees sugar hit three month low mcdo...\n",
      "8                    coke invests australian operation\n",
      "9    world coffee production fall below demand ico ...\n",
      "Name: Clean_Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('post regex documents', documents.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "MAX_NB_WORDS = 400000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = .00011\n",
    "BATCH_SIZE = 32\n",
    "DROPOUT_RATE = 0.45\n",
    "INNERLAYER_DROPOUT_RATE = 0.15\n",
    "np.random.seed(2032)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>level_3</th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price_Change</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Rate_of_Change</th>\n",
       "      <th>Clean_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>2007-01-02</td>\n",
       "      <td>1.1506</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>india earns more higher coffee exports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>2007-01-03</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022075</td>\n",
       "      <td>friesland raises stake indonesian subsidiary m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>2007-01-04</td>\n",
       "      <td>1.1451</td>\n",
       "      <td>-0.0309</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.026276</td>\n",
       "      <td>nymex announces start date soft commodity futu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>2007-01-05</td>\n",
       "      <td>1.1506</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004803</td>\n",
       "      <td>india largest coffee chain extends pakistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>2007-01-07</td>\n",
       "      <td>1.1506</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>honduran coffee sales ugandan coffee funds soy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  year  month  day level_3       Date   Price  \\\n",
       "0           0             0  2007      1    2   Title 2007-01-02  1.1506   \n",
       "1           1             1  2007      1    3   Title 2007-01-03  1.1760   \n",
       "2           2             2  2007      1    4   Title 2007-01-04  1.1451   \n",
       "3           3             3  2007      1    5   Title 2007-01-05  1.1506   \n",
       "4           4             4  2007      1    7   Title 2007-01-07  1.1506   \n",
       "\n",
       "   Price_Change  Direction  Rate_of_Change  \\\n",
       "0        0.0000          0        0.000000   \n",
       "1        0.0254          0        0.022075   \n",
       "2       -0.0309          0       -0.026276   \n",
       "3        0.0055          0        0.004803   \n",
       "4       -0.0000          0        0.000000   \n",
       "\n",
       "                                          Clean_Text  \n",
       "0             india earns more higher coffee exports  \n",
       "1  friesland raises stake indonesian subsidiary m...  \n",
       "2  nymex announces start date soft commodity futu...  \n",
       "3        india largest coffee chain extends pakistan  \n",
       "4  honduran coffee sales ugandan coffee funds soy...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "justcleandocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price_Change</th>\n",
       "      <th>Rate_of_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0254</td>\n",
       "      <td>0.022075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.0309</td>\n",
       "      <td>-0.026276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.004803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Price_Change  Rate_of_Change\n",
       "0        0.0000        0.000000\n",
       "1        0.0254        0.022075\n",
       "2       -0.0309       -0.026276\n",
       "3        0.0055        0.004803\n",
       "4       -0.0000        0.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "justlabels=thedata.drop(['Clean_Text'], axis=1)\n",
    "justlabels=pd.DataFrame(justlabels[[\"Price_Change\", 'Rate_of_Change']])\n",
    "justlabels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_index {0: 0, 1: 1, 2: 2}\n",
      "Found 3529 unique tokens\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Format our text samples and labels for use in Keras\n",
    "######################################################\n",
    "# Then we can format our text samples and labels into tensors that can be fed into a neural network. \n",
    "# Here we tokenize our source 'justcleandocs'\n",
    "# note that the values here are ultimately indexes to the actual words\n",
    "\n",
    "#convert text format\n",
    "justcleandocslist  = documents.values\n",
    "justcleandocslist[6]\n",
    "labels  = justlabels.values\n",
    "labels_index = {}\n",
    "#labels_index =  {0:0,1:1,2:2,3:3,4:4}\n",
    "labels_index =  {0:0,1:1,2:2}\n",
    "print('labels_index', labels_index)\n",
    "\n",
    "#tokenize the text\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(justcleandocslist) #tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(justcleandocslist) #sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index #word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens'.format(len(word_index)))\n",
    "#print('sequences first', sequences[0])\n",
    "\n",
    "#Pad sequences so that they all have the same length in a batch of input data \n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='pre', truncating='pre')\n",
    "sequences = None\n",
    "texts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor:  (1458, 10000)\n",
      "Shape of label tensor:  (1458, 2, 1)\n",
      "length of y_val 291\n",
      "shape of y_val (291, 2, 1)\n",
      "length of X_val 291\n",
      "shape of X_val (291, 10000)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor: ', data.shape)\n",
    "print('Shape of label tensor: ', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "X_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "X_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('length of y_val',len(y_val))\n",
    "print('shape of y_val',y_val.shape)\n",
    "print('length of X_val',len(X_val))\n",
    "print('shape of X_val',X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################\n",
    "# # Save Validation Set for Evaluation\n",
    "# ####################################\n",
    "# np.savetxt('y_val_3bin.txt', y_val, delimiter=',')\n",
    "# np.savetxt('X_val_3bin.txt', X_val,  fmt='%s', delimiter=',')\n",
    "# print('test and training sets saved to disk for later evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors to prepare the embedding layer...\n",
      "/home/jovyan/Capstone\n",
      "Loading Glove Model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-3b019efd8ba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading Glove Model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mgloveFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'C:\\\\glove\\\\glove6B300d.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgloveFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE_NONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Preparing the embedding layer\n",
    "########################################\n",
    "\n",
    "#load in word vectors from glove reference global English data set\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "# see more reference links at bottom\n",
    "\n",
    "print('Loading word vectors to prepare the embedding layer...')\n",
    "print(os.getcwd())\n",
    "\n",
    "embeddings_index = {}\n",
    "print('Loading Glove Model...')\n",
    "gloveFile = 'C:\\\\glove\\\\glove6B300d.txt'\n",
    "words = pd.read_table(gloveFile, sep=\" \", header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "print(words.head(5))\n",
    "print('shape of glove model',words.shape)\n",
    "\n",
    "wordkeys=words.iloc[:,0]\n",
    "print('wordkeys type of file', type(wordkeys))\n",
    "words2 = words.rename(columns={ words.columns[0]: \"words\" })\n",
    "words2['words'].apply(str)\n",
    "#print(words2.dtypes)\n",
    "\n",
    "embeddings_index = words2.set_index('words').T.to_dict('list')\n",
    "\n",
    "#print(dict(list(embeddings_index.items())[0:2]))\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))\n",
    "#usage of pandas function dataFrame.to_dict(outtype='dict') outtype : str {‘dict’, ‘list’, ‘series’}\n",
    "\n",
    "\n",
    "#################################\n",
    "#Build the embedding matrix\n",
    "#################################\n",
    "\n",
    "print('Building Embedding Matrix...')\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
